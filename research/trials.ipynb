{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import os \n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path=\"test_repo/\"\n",
    "repo=Repo.clone_from(\"https://github.com/vishnugnair/MEDICAL-CHATBOT.git\",to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],  # Now processing JavaScript files\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)  # Now parsing JS\n",
    ")\n",
    "\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='from flask import Flask, render_template, request\\nimport os\\nfrom dotenv import load_dotenv\\n\\n# LangChain + Pinecone\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_openai import OpenAI\\n\\n# Local modules\\nfrom store_index import create_index_if_not_exists, load_or_create_docsearch, index_name\\nfrom src.prompt import prompt\\n\\napp = Flask(__name__)\\n\\nload_dotenv()\\nOPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\\n\\n# 1. Create or verify Pinecone index\\ncreate_index_if_not_exists()\\n\\n# 2. Load documents and create docsearch\\ndocsearch, embeddings = load_or_create_docsearch()\\n\\n# 3. Build retriever\\nretriever = docsearch.as_retriever(search_type=\"similarity\", search_kwags={\"k\": 3})\\n\\n# 4. Initialize OpenAI LLM\\nllm = OpenAI(api_key=OPENAI_API_KEY, temperature=0.4, max_tokens=500)\\n\\n# 5. Create Q&A chain and RAG chain\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n@app.route(\\'/\\')\\ndef index():\\n    \"\"\"\\n    Render the home page (index.html).\\n    \"\"\"\\n    return render_template(\\'index.html\\', response=None)\\n\\n@app.route(\\'/chat\\', methods=[\\'POST\\'])\\ndef chat():\\n    \"\"\"\\n    Handle the user query from the form, run the RAG chain, and return the answer.\\n    \"\"\"\\n    user_query = request.form.get(\\'question\\')\\n    if not user_query:\\n        return render_template(\\'index.html\\', response=\"No question provided.\")\\n\\n    # Run the RAG chain with the user query\\n    response = rag_chain.invoke({\"input\": user_query})\\n    answer = response[\"answer\"]\\n\\n    return render_template(\\'index.html\\', response=answer, query=user_query)\\n\\nif __name__ == \\'__main__\\':\\n    # Run on localhost:8080\\n    app.run(host=\\'0.0.0.0\\', port=8080, debug=True)\\n', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"from setuptools import find_packages, setup\\n\\nsetup(\\n    name='Generative AI Project',\\n    version='0.0.0',\\n    author='Vishnu G Nair',\\n    author_email='nairvishnu866@gmail.com',\\n    packages=find_packages(),\\n    install_requires=[]\\n)\\n\", metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()  # Load environment variables from .env\\n\\n# Pinecone + LangChain + Embeddings\\nfrom pinecone import Pinecone, ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\\n\\n# Local imports from helper.py\\nfrom src.helper import load_pdf_file, text_split\\n\\n# Load API keys from .env\\nPINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\\nOPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\\n\\n# Check if API keys are loaded correctly\\nif not PINECONE_API_KEY:\\n    raise ValueError(\"Error: PINECONE_API_KEY not found in environment variables.\")\\nif not OPENAI_API_KEY:\\n    raise ValueError(\"Error: OPENAI_API_KEY not found in environment variables.\")\\n\\n# Initialize Pinecone\\npc = Pinecone(api_key=PINECONE_API_KEY)\\nindex_name = \"quickstart\"\\n\\ndef create_index_if_not_exists():\\n    \"\"\"\\n    Creates a Pinecone serverless index if it doesn\\'t already exist.\\n    \"\"\"\\n    try:\\n        # Check existing indexes before creating a new one\\n        existing_indexes = [index[\"name\"] for index in pc.list_indexes()]\\n        \\n        if index_name not in existing_indexes:\\n            pc.create_index(\\n                name=index_name,\\n                dimension=384,  # Must match the embedding dimension\\n                metric=\"cosine\",\\n                spec=ServerlessSpec(\\n                    cloud=\"aws\",\\n                    region=\"us-east-1\"\\n                )\\n            )\\n            print(f\"✅ Index \\'{index_name}\\' created.\")\\n        else:\\n            print(f\"⚠️ Index \\'{index_name}\\' already exists. Skipping creation.\")\\n    \\n    except Exception as e:\\n        print(f\"⚠️ Index creation skipped due to error: {e}\")\\n\\ndef load_or_create_docsearch():\\n    \"\"\"\\n    Loads PDF data, splits it, and creates (or reuses) a Pinecone vector store.\\n    Returns both the docsearch and the embeddings object.\\n    \"\"\"\\n    # 1. Load data from PDF folder\\n    documents = load_pdf_file(\\'Data/\\')\\n\\n    # 2. Split into text chunks\\n    text_chunks = text_split(documents)\\n\\n    # 3. Create embeddings\\n    embeddings = HuggingFaceEmbeddings(model_name=\\'sentence-transformers/all-MiniLM-L6-v2\\')\\n\\n    # 4. Check if index exists before creating docsearch\\n    existing_indexes = [index[\"name\"] for index in pc.list_indexes()]\\n    \\n    if index_name in existing_indexes:\\n        print(f\"⚠️ Using existing Pinecone index: {index_name}\")\\n        docsearch = PineconeVectorStore.from_existing_index(index_name, embedding=embeddings)\\n    else:\\n        print(f\"✅ Creating new Pinecone index and storing embeddings.\")\\n        docsearch = PineconeVectorStore.from_documents(\\n            documents=text_chunks,\\n            index_name=index_name,\\n            embedding=embeddings,\\n        )\\n\\n    return docsearch, embeddings\\n', metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s: %(message)s\\')\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"requirements.txt\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\"\\n]\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n    if filedir != \"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory: {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n        logging.info(f\"Creating empty file: {filepath}\")\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")\\n', metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom langchain.document_loaders import DirectoryLoader, PyPDFLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\ndef load_pdf_file(folder_path: str):\\n    \"\"\"\\n    Loads all PDF files from the given folder using PyPDFLoader.\\n    \"\"\"\\n    loader = DirectoryLoader(\\n        folder_path,\\n        glob=\"*.pdf\",\\n        loader_cls=PyPDFLoader\\n    )\\n    return loader.load()\\n\\ndef text_split(extracted_data):\\n    \"\"\"\\n    Splits the extracted documents into smaller text chunks for processing.\\n    \"\"\"\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks = text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n', metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from langchain_core.prompts import ChatPromptTemplate\\n\\n# System prompt template\\nsystem_prompt = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you don’t know the answer, say that you \"\\n    \"don’t know. Use three sentences maximum and keep the \"\\n    \"answer concise.\\\\n\\\\n\"\\n    \"{context}\"\\n)\\n\\n# Create a ChatPromptTemplate\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", system_prompt),\\n    (\"human\", \"{input}\")\\n])\\n', metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=20\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb=Chroma.from_documents(texts,embedding=embeddings,persist_directory=\"./db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":8})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"How is RAG achieved in my app?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG (Retriever, Answerer, and Generator) is achieved in your app through the RAG chain that is created using the `rag_chain` variable. The RAG chain is initiated by invoking the chain with the user query using the `rag_chain.invoke({\"input\": user_query})` function. This chain integrates retriever, answerer, and generator components to process the user query and provide a response. The response is then used to render the template with the answer and the user query.\n"
     ]
    }
   ],
   "source": [
    "result = qa({\"question\": question, \"chat_history\": []})\n",
    "print(result[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
